{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Datasets"
      ],
      "metadata": {
        "id": "vmVuRLusYg9U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u16qFVAKSfvn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "415d8673-5847-4f93-f511-77ec78d716e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello!', 'How are you?', 'I am fine.']\n",
            "['Hello', '!', 'How', 'are', 'you', '?', 'I', 'am', 'fine', '.']\n",
            "[('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('is', 'VBZ'), ('fascinating', 'VBG'), ('.', '.')]\n",
            "(S (GPE Amit/NNP) is/VBZ looking/VBG (PERSON Good/NNP) ./.)\n",
            "['A', 'a', 'aa', 'aal', 'aalii', 'aam', 'Aani', 'aardvark', 'aardwolf', 'Aaron']\n",
            "{'neg': 0.0, 'neu': 0.435, 'pos': 0.565, 'compound': 0.5983}\n"
          ]
        }
      ],
      "source": [
        "#pip install nltk\n",
        "#DOWNLOAD THE DATASET\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "\n",
        "\n",
        "# punkt:\n",
        "\n",
        "\n",
        "# Punk Example to set identify and seperate the sentences\n",
        "# Purpose: punkt is a tokenizer that splits text into sentences and words.\n",
        "# Use Case: It’s used for sentence splitting and word tokenization, essential steps in text preprocessing.\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "text = \"Hello! How are you? I am fine.\"\n",
        "print(sent_tokenize(text))  # Output: ['Hello!', 'How are you?', 'I am fine.']\n",
        "print(word_tokenize(text))  # Output: ['Hello', '!', 'How', 'are', 'you', '?', 'I', 'am', 'fine', '.']\n",
        "\n",
        "\n",
        "\n",
        "# averaged_perceptron_tagger:\n",
        "\n",
        "\n",
        "# Purpose: This is a part-of-speech (POS) tagger.\n",
        "# Use Case: It tags words in a sentence with their respective parts of speech (e.g., noun, verb, adjective).\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "words = word_tokenize(\"Natural language processing is fascinating.\")\n",
        "print(pos_tag(words))  # Output: [('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('is', 'VBZ'), ('fascinating', 'VBG'), ('.', '.')]\n",
        "\n",
        "\n",
        "# maxent_ne_chunker:\n",
        "\n",
        "\n",
        "# Purpose: This is used for Named Entity Recognition (NER).\n",
        "# Use Case: It identifies named entities (e.g., person names, organizations, locations) in text.\n",
        "from nltk import pos_tag, ne_chunk\n",
        "from nltk.tokenize import word_tokenize\n",
        "words = word_tokenize(\"Amit is looking Good.\")\n",
        "pos_tags = pos_tag(words)\n",
        "print(ne_chunk(pos_tags))  # Output: (S Apple/NNP is/VBZ looking/VBG at/IN buying/VBG U.K./NNP startup/NN for/IN $/$ 1/CD billion/CD ./.)\n",
        "\n",
        "\n",
        "# words:\n",
        "\n",
        "# Purpose: This dataset contains a list of English words.\n",
        "# Use Case: It’s often used for spell checking, word validation, or other linguistic tasks.\n",
        "\n",
        "from nltk.corpus import words\n",
        "print(words.words()[:10])  # Output: ['A', 'a', 'aa', 'aal', 'aalii', 'aam', 'Aani', 'aardvark', 'aardwolf', 'Aaron']\n",
        "\n",
        "\n",
        "# vader_lexicon:\n",
        "#\n",
        "# Purpose: VADER (Valence Aware Dictionary and sEntiment Reasoner) is a sentiment analysis tool.\n",
        "# Use Case: It’s used for sentiment analysis, particularly in social media text where emojis, slang, and punctuation are important.\n",
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "text = \"NLTK is a fantastic library!\"\n",
        "print(sid.polarity_scores(text))  # Output: {'neg': 0.0, 'neu': 0.245, 'pos': 0.755, 'compound': 0.7023}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization ........................................................"
      ],
      "metadata": {
        "id": "ReJSDnQvYdb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenization is the process of splitting text into individual words or sentences.\n",
        "# his line imports two functions, word_tokenize and sent_tokenize, from the nltk.tokenize module.\n",
        "# These functions are used for tokenizing text into words and sentences, respectively.\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "text = \"Hello, how are you? I hope you are doing well. NLP is fun!\"\n",
        "\n",
        "# Sentence Tokenization\n",
        "# sent_tokenize(text): This function splits the input text into individual sentences based on punctuation marks\n",
        "# (like periods, exclamation marks, and question marks).\n",
        "# Example:\n",
        "# Input text: \"Hello, how are you? I hope you are doing well. NLP is fun!\"\n",
        "# Output sentences: ['Hello, how are you?', 'I hope you are doing well.', 'NLP is fun!']\n",
        "# The sentences list contains each sentence as a separate string.\n",
        "\n",
        "sentences = sent_tokenize(text)\n",
        "print(\"Sentences:\", sentences)\n",
        "\n",
        "\n",
        "\n",
        "# word_tokenize(text): This function splits the input text into individual words and punctuation marks.\n",
        "# Example:\n",
        "# Input text: \"Hello, how are you? I hope you are doing well. NLP is fun!\"\n",
        "# Output words: ['Hello', ',', 'how', 'are', 'you', '?', 'I', 'hope', 'you', 'are', 'doing', 'well', '.', 'NLP', 'is', 'fun', '!']\n",
        "# The words list contains each word and punctuation mark as a separate string element.\n",
        "# Word Tokenization\n",
        "words = word_tokenize(text)\n",
        "print(\"Words:\", words)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lpm7LhkWYbZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# POS Tagging ........................................................................."
      ],
      "metadata": {
        "id": "GW4SYPvnuWLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Part-of-Speech (POS) tagging is the process of marking up a word in a text as corresponding to a particular part of speech.\n",
        "# This imports the necessary functions for tokenization (word_tokenize) and part-of-speech tagging (pos_tag) from NLTK (Natural Language Toolkit).\n",
        "# POS tagging assigns a specific part-of-speech tag to each word in the input text.\n",
        "# python\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "text = \"Hello, how are you? I hope you are doing well. NLP is fun!\"\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# POS Tagging\n",
        "pos_tags = pos_tag(words)\n",
        "print(\"POS Tags:\", pos_tags)\n"
      ],
      "metadata": {
        "id": "eEBDtQxsu4WV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Name Entity Tagging"
      ],
      "metadata": {
        "id": "CUaPcti3u5dG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Named Entity Recognition is the process of identifying and categorizing key information (entities) in text.\n",
        "# This imports the necessary NLTK modules and functions for Named Entity Recognition (NER),\n",
        "# including ne_chunk for performing NER, word_tokenize for tokenizing words, and pos_tag for part-of-speech tagging.\n",
        "\n",
        "from nltk import ne_chunk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "text = \"Barack Obama was born in Hawaii. He was elected president in 2008.\"\n",
        "\n",
        "# word_tokenize(text): Tokenizes the input text into individual words and punctuation marks.\n",
        "# Example:\n",
        "# Input text: \"Barack Obama was born in Hawaii. He was elected president in 2008.\"\n",
        "# Output words: ['Barack', 'Obama', 'was', 'born',  '.']\n",
        "\n",
        "# pos_tag(words): Assigns part-of-speech tags to each tokenized word.\n",
        "# Example (for the tokenized words above):\n",
        "# Output POS tags: [('Barack', 'NNP'), ('Obama', 'NNP'),  ('.', '.')]\n",
        "# Each tuple pairs a word/token with its corresponding part-of-speech tag.\n",
        "\n",
        "\n",
        "words = word_tokenize(text)\n",
        "pos_tags = pos_tag(words)\n",
        "\n",
        "# Named Entity Recognition\n",
        "# ne_chunk(pos_tags): Applies Named Entity Recognition to the part-of-speech tagged words.\n",
        "# Example (based on the POS tags above):\n",
        "# Output named entities: Tree('S', [Tree('PERSON', [('Barack', 'NNP'), ('Obama', 'NNP')]), ('was', 'VBD'), ('born', 'VBN'), ('in', 'IN'),....\n",
        "# ne_chunk organizes recognized entities into a nested tree structure (Tree objects) where entities like persons (PERSON) and\n",
        "# geopolitical entities (GPE) are labeled accordingly.\n",
        "\n",
        "named_entities = ne_chunk(pos_tags)\n",
        "print(\"Named Entities:\", named_entities)\n"
      ],
      "metadata": {
        "id": "7UP4u3EUvNCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis"
      ],
      "metadata": {
        "id": "6YchWYV3vR3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sentiment analysis is the process of determining the sentiment or emotion expressed in a text.\n",
        "# This line imports SentimentIntensityAnalyzer from NLTK('s sentiment module. '\n",
        "# SentimentIntensityAnalyzer is a pre-built tool in NLTK that helps analyze and quantify the sentiment expressed in a piece of text.)\n",
        "\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "text = \"I love this product! It's amazing.\"\n",
        "\n",
        "# Creates an instance of SentimentIntensityAnalyzer. This initializes the sentiment analyzer object that will be used to analyze the sentiment of the text.\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# sia.polarity_scores(text): Analyzes the sentiment of the input text and returns a dictionary of sentiment scores.\n",
        "# Example:\n",
        "# Input text: \"I love this product! It's amazing.\"\n",
        "# Output sentiment scores: {'neg': 0.0, 'neu': 0.297, 'pos': 0.703, 'compound': 0.7351}\n",
        "sentiment = sia.polarity_scores(text)\n",
        "print(\"Sentiment:\", sentiment)\n"
      ],
      "metadata": {
        "id": "HRyWM-ZYvcw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP ALl in One"
      ],
      "metadata": {
        "id": "EYrrcHXjvemh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Merged together\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk import pos_tag, ne_chunk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Sample text\n",
        "text = \"Barack Obama was born in Hawaii. He was elected president in 2008. I love this product! It's amazing.\"\n",
        "\n",
        "# Sentence Tokenization\n",
        "sentences = sent_tokenize(text)\n",
        "print(\"Sentences:\", sentences)\n",
        "print()\n",
        "\n",
        "# Word Tokenization\n",
        "words = word_tokenize(text)\n",
        "print(\"Words:\", words)\n",
        "print()\n",
        "\n",
        "# POS Tagging\n",
        "pos_tags = pos_tag(words)\n",
        "print(\"POS Tags:\", pos_tags)\n",
        "print()\n",
        "\n",
        "\n",
        "# Named Entity Recognition\n",
        "named_entities = ne_chunk(pos_tags)\n",
        "print(\"Named Entities:\", named_entities)\n",
        "print()\n",
        "\n",
        "\n",
        "# Sentiment Analysis\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "sentiment = sia.polarity_scores(text)\n",
        "print(\"Sentiment:\", sentiment)\n",
        "print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jJwGUfRvj7c",
        "outputId": "836c2a5d-f5ec-49ff-c290-41fcd7030110"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences: ['Barack Obama was born in Hawaii.', 'He was elected president in 2008.', 'I love this product!', \"It's amazing.\"]\n",
            "\n",
            "Words: ['Barack', 'Obama', 'was', 'born', 'in', 'Hawaii', '.', 'He', 'was', 'elected', 'president', 'in', '2008', '.', 'I', 'love', 'this', 'product', '!', 'It', \"'s\", 'amazing', '.']\n",
            "\n",
            "POS Tags: [('Barack', 'NNP'), ('Obama', 'NNP'), ('was', 'VBD'), ('born', 'VBN'), ('in', 'IN'), ('Hawaii', 'NNP'), ('.', '.'), ('He', 'PRP'), ('was', 'VBD'), ('elected', 'VBN'), ('president', 'NN'), ('in', 'IN'), ('2008', 'CD'), ('.', '.'), ('I', 'PRP'), ('love', 'VBP'), ('this', 'DT'), ('product', 'NN'), ('!', '.'), ('It', 'PRP'), (\"'s\", 'VBZ'), ('amazing', 'JJ'), ('.', '.')]\n",
            "\n",
            "Named Entities: (S\n",
            "  (PERSON Barack/NNP)\n",
            "  (PERSON Obama/NNP)\n",
            "  was/VBD\n",
            "  born/VBN\n",
            "  in/IN\n",
            "  (GPE Hawaii/NNP)\n",
            "  ./.\n",
            "  He/PRP\n",
            "  was/VBD\n",
            "  elected/VBN\n",
            "  president/NN\n",
            "  in/IN\n",
            "  2008/CD\n",
            "  ./.\n",
            "  I/PRP\n",
            "  love/VBP\n",
            "  this/DT\n",
            "  product/NN\n",
            "  !/.\n",
            "  It/PRP\n",
            "  's/VBZ\n",
            "  amazing/JJ\n",
            "  ./.)\n",
            "\n",
            "Sentiment: {'neg': 0.0, 'neu': 0.644, 'pos': 0.356, 'compound': 0.8516}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Et32xJq-vklJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}